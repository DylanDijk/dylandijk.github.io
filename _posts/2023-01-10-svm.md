---
title: Support Vector Machines
date: 2023-01-09 09:42:00 +0000
categories: [Statistics, Machine Learning]
tags: [SVM, ML]
img_path: /assets/img/figures/
math: true
---

# SVM

## Introduction

We are in the binary classification setup, and we want to create a linear decision rule that when given a feature vector we predict the class it belongs to. What we mean by a linear decision rule, is that the decision rule will be based on some activation function of affine combinantion of our features.

In particular, we will define our decision rule as:

$$\begin{align} \label{eq:binary linear classifier}
    f(\bx) &= 
     \begin{cases}
       +1, &\quad g(\bx;\bw) \geq 0 
       \\
       -1, &\quad g(\bx;\bw) < 0 
     \end{cases} \\
     \text{Where } g(\bx;\bw) &= \langle \bw',\bx \rangle + w_0 \nonumber
\end{align}$$

The perceptron algorithm 
to choose the value of w uses gradient ascent iteratively on observations that are miss-classified. If
the data is linearly separable it can be shown that the algorithm will find a solution (decision rule
where all points correctly classified) in a finite number of steps.

However, if the data is linearly separable there will be an infinite number of choices for a decision
boundary. We want a decision boundary that generalizes best to unseen data, Support Vector
Machines tries to solve this problem.


![svm-diagram](SVM-diagram.svg)
_SVM_

