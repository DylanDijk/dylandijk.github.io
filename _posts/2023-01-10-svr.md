---
title: Support Vector Regression
date: 2023-01-09 09:42:00 +0000
categories: [Statistics, Machine Learning]
tags: [SVM, ML]
img_path: /assets/img/figures/
math: true
---

In this post I cover the fundamentals of Support Vector Regression.  
A high level summary, is that a SVR is a ...

**Main references**:
  - [PRML](https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf) by Christoper Bishop - Section 7


# Epsilon-Inensistive Loss Function

In standard linear regression we use the squared error loss, which can be motivated by making the probabilistic assumption that the outcome variables are normally distributed conditionally given the predictors.

A way to view support vector regression (SVR) is by introducing the $\boldsymbol{\epsilon}$**-insensistive loss function**.  
This is defined below:

$$\begin{align} \label{eq:binary linear classifier}
    E_{\epsilon}(y - g(\bx;\bw)) =
     \begin{cases}
       0, \quad &|g(\bx;\bw) - y| < \epsilon
       \\
       |g(\bx;\bw) - y| - \epsilon, &\text{otherwise}
     \end{cases}
\end{align}$$

In words, this loss function only punishes incorrect predictions when the discrepancy between the actual value and the predicted value is larger than some choice of threshold $\epsilon$.


## Regularised Regression

The objective function for SVR can be defined as regularised regression, using the $\boldsymbol{\epsilon}$**-insensistive loss function**.  
This is given by:

$$\sum_{i = 1}^{n}{E_{\epsilon}(y_i - g(\bx_i;\bw))} + \lambda||\bw'||^2$$

# Introducing Slack Variables

Minimising the objective function given above, can be rewritten as a constrained optimisation problem by introducing slack variables $\xi$, $\hat{\xi}$.

This is given by:

$$\begin{align}
\underset{\xi,\hat{\xi},\bw'} \argmin{\sum_{i = 1}^{n}}{(\xi_i + \hat{\xi_i})} + \lambda||\bw'||^2  \\
{\text{subject to:}}\\
 \quad y_i - g(\bx_i;\bw') \leq \epsilon + \xi_i \\
g(\bx_i;\bw') - y_i \leq \epsilon + \hat{\xi}_i \\
\xi_i, \hat{\xi}_i \geq 0
\end{align}$$

## Dual Formulation

In the [SVM post](https://dylandijk.github.io/posts/svm/) I gave the dual form of the soft margin classifier, I then showed that this allows us to implement the Kernel trick. The Kernel trick is a way of applying a transformation to our features, but avoiding its direct computation. Therefore allowing us to transform to very high dimensional spaces, even infinite dimensional spaces.

The dual formulation of SVR is given by:

