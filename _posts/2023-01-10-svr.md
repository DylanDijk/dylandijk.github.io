---
title: Support Vector Regression
date: 2023-01-09 09:42:00 +0000
categories: [Statistics, Machine Learning]
tags: [SVM, ML]
img_path: /assets/img/figures/
math: true
---

In this post I cover the fundamentals of Support Vector Regression. A high level summary, is that a SVR is a ...

**Main references**:
  - [PRML](https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf) by Christoper Bishop - Section 7

# Introduction

## $\epsilon$-Inensistive Loss Function

In standard linear regression we use the squared error loss, which can be motivated by making the probabilistic assumption that the outcome variables are normally distributed given the predictors.

A way to view support vector regression (SVR) is by introducing the $\epsilon$-insensistive loss function. This is defined below:

$$\begin{align} \label{eq:binary linear classifier}
    E_{\epsilon}(y - g(\bx;\bw)) &= 
     \begin{cases}
       0, &\quad g(\bx;\bw) \geq 0 
       \\
       -1, &\quad g(\bx;\bw) < 0 
     \end{cases} \\
     \text{Where } g(\bx;\bw) &= \langle \bw',\bx \rangle + w_0 \nonumber
\end{align}$$



